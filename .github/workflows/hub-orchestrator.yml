# ============================================================================
# CI/CD Hub - Main Orchestrator
# ============================================================================
# This workflow orchestrates CI/CD for all configured repositories.
# It reads config, triggers builds, and aggregates reports.
#
# Triggers:
#   - Manual dispatch (workflow_dispatch)
#   - Scheduled (e.g., nightly)
#   - When config changes
# ============================================================================

name: Hub Orchestrator

on:
  workflow_dispatch:
    inputs:
      repos:
        description: 'Comma-separated repo names (empty = all)'
        required: false
        type: string

  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'

  push:
    branches: [main, master]
    paths:
      - 'config/**'
      - '.github/workflows/hub-orchestrator.yml'

permissions:
  contents: read
  actions: write

# Prevent concurrent runs for the same ref
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================================================
  # Load Configuration
  # ============================================================================
  load-config:
    name: Load Repository Config
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.config.outputs.matrix }}
      repo_count: ${{ steps.config.outputs.count }}

    steps:
      - name: Checkout Hub
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Dependencies
        run: pip install pyyaml jsonschema

      - name: Generate Build Matrix
        id: config
        run: |
          python << 'EOF'
          import json
          import os
          import sys
          from pathlib import Path
          from scripts.load_config import load_config, generate_workflow_inputs

          hub_root = Path(".")
          repos_dir = hub_root / "config" / "repos"
          matrix_entries = []

          # Optional filter
          filter_repos = os.environ.get("INPUT_REPOS", "").strip()
          filter_list = [r.strip() for r in filter_repos.split(",")] if filter_repos else None

          for config_file in sorted(repos_dir.glob("*.yaml")):
              repo_name = config_file.stem
              try:
                  cfg = load_config(repo_name=repo_name, hub_root=hub_root)
              except SystemExit:
                  raise
              except Exception as exc:
                  print(f"Failed to load config {config_file}: {exc}", file=sys.stderr)
                  sys.exit(1)

              repo_info = cfg.get("repo", {})
              if not (repo_info.get("owner") and repo_info.get("name") and (repo_info.get("language") or cfg.get("language"))):
                  print(f"Skipping {config_file}: missing repo.owner/name/language")
                  continue
              if filter_list and repo_info.get("name", repo_name) not in filter_list:
                  continue

              inputs = generate_workflow_inputs(cfg)
              language = repo_info.get("language", cfg.get("language", "java"))
              # Default dispatch workflow: prefer new hub-*-ci.yml (reusable workflow callers)
              # Repos without hub-*-ci.yml should set dispatch_workflow in config to old name
              # Migration: old repos use *-ci-dispatch.yml, new repos use hub-*-ci.yml
              default_workflow = "hub-java-ci.yml" if language == "java" else "hub-python-ci.yml"
              entry = {
                  "config_file": str(config_file),
                  "config_basename": config_file.stem,  # Unique per config for artifact naming
                  "name": repo_info.get("name", repo_name),
                  "owner": repo_info.get("owner", "jguida941"),
                  "language": language,
                  "default_branch": repo_info.get("default_branch", "main"),
                  "subdir": repo_info.get("subdir", ""),
                  "dispatch_enabled": repo_info.get("dispatch_enabled", True),
                  "dispatch_workflow": repo_info.get("dispatch_workflow", default_workflow),
                  "run_group": repo_info.get("run_group", "full"),
              }
              entry.update(inputs)
              matrix_entries.append(entry)

          matrix = {"include": matrix_entries}

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"matrix={json.dumps(matrix)}\n")
              f.write(f"count={len(matrix_entries)}\n")

          print(f"Found {len(matrix_entries)} repositories:")
          for r in matrix_entries:
              print(f"  - {r['owner']}/{r['name']} ({r['language']}) run_group={r.get('run_group','full')}")
          EOF
        env:
          INPUT_REPOS: ${{ inputs.repos }}

      - name: Summary
        run: |
          echo "## Hub Orchestrator" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Repositories to build:** ${{ steps.config.outputs.count }}" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # Trigger Builds for Each Repo
  # ============================================================================
  trigger-builds:
    name: Build ${{ matrix.config_basename }}
    runs-on: ubuntu-latest
    needs: load-config
    if: needs.load-config.outputs.repo_count > 0
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.load-config.outputs.matrix) }}

    steps:
      - name: Checkout Hub
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Dependencies
        run: pip install pyyaml

      - name: Load Repo Config
        id: repo-config
        env:
          MATRIX_CONFIG_FILE: ${{ matrix.config_file }}
          MATRIX_LANGUAGE: ${{ matrix.language }}
          MATRIX_SUBDIR: ${{ matrix.subdir }}
          MATRIX_OWNER: ${{ matrix.owner }}
          MATRIX_REPO: ${{ matrix.name }}
          MATRIX_BRANCH: ${{ matrix.default_branch }}
          MATRIX_DISPATCH_WORKFLOW: ${{ matrix.dispatch_workflow }}
          MATRIX_DISPATCH_ENABLED: ${{ matrix.dispatch_enabled }}
          REPO_TOKEN: ${{ secrets.HUB_DISPATCH_TOKEN || secrets.GITHUB_TOKEN }}
        run: |
          python << 'EOF'
          import base64
          import json
          import os
          import urllib.error
          import urllib.request
          import yaml
          from pathlib import Path

          # Load defaults
          defaults = {}
          defaults_file = Path("config/defaults.yaml")
          if defaults_file.exists():
              with open(defaults_file) as f:
                  defaults = yaml.safe_load(f) or {}

          # Load repo-specific config (using env var to prevent injection)
          repo_config = {}
          config_file = Path(os.environ.get("MATRIX_CONFIG_FILE", ""))
          if config_file.exists():
              with open(config_file) as f:
                  repo_config = yaml.safe_load(f) or {}

          # Load repo-local .ci-hub.yml (highest precedence) via GitHub API
          repo_local = {}
          owner = os.environ.get("MATRIX_OWNER", "").strip()
          repo = os.environ.get("MATRIX_REPO", "").strip()
          branch = os.environ.get("MATRIX_BRANCH", "").strip() or "main"
          token = os.environ.get("REPO_TOKEN", "").strip()
          if owner and repo and token:
              url = f"https://api.github.com/repos/{owner}/{repo}/contents/.ci-hub.yml?ref={branch}"
              req = urllib.request.Request(
                  url,
                  headers={
                      "Authorization": f"token {token}",
                      "Accept": "application/vnd.github+json",
                  },
              )
              try:
                  with urllib.request.urlopen(req, timeout=10) as resp:
                      data = json.loads(resp.read().decode("utf-8"))
                      content = data.get("content", "")
                      if content:
                          decoded = base64.b64decode(content).decode("utf-8")
                          repo_local = yaml.safe_load(decoded) or {}
              except urllib.error.HTTPError as exc:
                  if exc.code != 404:
                      print(f"Warning: failed to fetch .ci-hub.yml from {owner}/{repo}: {exc}")
              except Exception as exc:
                  print(f"Warning: error reading repo-local .ci-hub.yml: {exc}")

          if repo_local and not isinstance(repo_local, dict):
              print("Warning: repo-local .ci-hub.yml is not a mapping; ignoring.")
              repo_local = {}

          # Prevent repo-local config from overriding repo identity and dispatch settings
          # These are controlled by the hub, not the repo
          if isinstance(repo_local, dict):
              repo_block = repo_local.get("repo")
              if isinstance(repo_block, dict):
                  repo_block.pop("owner", None)
                  repo_block.pop("name", None)
                  repo_block.pop("language", None)
                  repo_block.pop("dispatch_workflow", None)
                  repo_block.pop("dispatch_enabled", None)
                  repo_local["repo"] = repo_block

          # Deep merge (repo overrides defaults)
          def deep_merge(base, override):
              result = base.copy()
              for key, value in override.items():
                  if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                      result[key] = deep_merge(result[key], value)
                  else:
                      result[key] = value
              return result

          merged = deep_merge(defaults, repo_config)
          if repo_local:
              merged = deep_merge(merged, repo_local)
          language = os.environ.get("MATRIX_LANGUAGE", "java")
          merged["language"] = language

          # Extract workflow inputs based on language
          inputs = {"language": language}

          # Optional subdir for monorepos
          repo_meta = merged.get("repo", {}) if isinstance(merged.get("repo"), dict) else {}
          subdir = (repo_meta.get("subdir") or os.environ.get("MATRIX_SUBDIR", "")).strip()
          if subdir:
              inputs["workdir"] = subdir

          # Bundle thresholds into threshold_overrides_yaml (ADR-0024)
          global_thresholds = merged.get("thresholds", {})
          threshold_bundle = {
              "max_critical_vulns": global_thresholds.get("max_critical_vulns", 0),
              "max_high_vulns": global_thresholds.get("max_high_vulns", 0),
          }

          if language == "java":
              java = merged.get("java", {})
              tools = java.get("tools", {})

              inputs["java_version"] = java.get("version", "21")
              inputs["build_tool"] = java.get("build_tool", "maven")
              inputs["run_jacoco"] = tools.get("jacoco", {}).get("enabled", True)
              inputs["run_checkstyle"] = tools.get("checkstyle", {}).get("enabled", True)
              inputs["run_spotbugs"] = tools.get("spotbugs", {}).get("enabled", True)
              inputs["run_owasp"] = tools.get("owasp", {}).get("enabled", True)
              inputs["use_nvd_api_key"] = tools.get("owasp", {}).get("use_nvd_api_key", True)
              inputs["run_pitest"] = tools.get("pitest", {}).get("enabled", True)
              inputs["run_jqwik"] = tools.get("jqwik", {}).get("enabled", False)
              inputs["run_codeql"] = tools.get("codeql", {}).get("enabled", True)
              inputs["run_pmd"] = tools.get("pmd", {}).get("enabled", True)
              inputs["run_semgrep"] = tools.get("semgrep", {}).get("enabled", True)
              inputs["run_trivy"] = tools.get("trivy", {}).get("enabled", True)
              inputs["run_docker"] = tools.get("docker", {}).get("enabled", False)

              # Java-specific thresholds
              threshold_bundle["coverage_min"] = tools.get("jacoco", {}).get("min_coverage", 70)
              threshold_bundle["mutation_score_min"] = tools.get("pitest", {}).get("min_mutation_score", 70)
              threshold_bundle["owasp_cvss_fail"] = tools.get("owasp", {}).get("fail_on_cvss", 7)
              threshold_bundle["max_checkstyle_errors"] = tools.get("checkstyle", {}).get("max_errors", 0)
              threshold_bundle["max_spotbugs_bugs"] = tools.get("spotbugs", {}).get("max_bugs", 0)
              threshold_bundle["max_pmd_violations"] = tools.get("pmd", {}).get("max_violations", 0)
              threshold_bundle["max_semgrep_findings"] = tools.get("semgrep", {}).get("max_findings", 0)

          elif language == "python":
              python = merged.get("python", {})
              tools = python.get("tools", {})

              inputs["python_version"] = python.get("version", "3.12")
              inputs["run_pytest"] = tools.get("pytest", {}).get("enabled", True)
              inputs["run_ruff"] = tools.get("ruff", {}).get("enabled", True)
              inputs["run_bandit"] = tools.get("bandit", {}).get("enabled", True)
              inputs["run_pip_audit"] = tools.get("pip_audit", {}).get("enabled", True)
              inputs["run_codeql"] = tools.get("codeql", {}).get("enabled", True)
              inputs["run_mypy"] = tools.get("mypy", {}).get("enabled", False)
              inputs["run_black"] = tools.get("black", {}).get("enabled", True)
              inputs["run_isort"] = tools.get("isort", {}).get("enabled", True)
              inputs["run_mutmut"] = tools.get("mutmut", {}).get("enabled", True)
              inputs["run_hypothesis"] = tools.get("hypothesis", {}).get("enabled", True)
              inputs["run_semgrep"] = tools.get("semgrep", {}).get("enabled", True)
              inputs["run_trivy"] = tools.get("trivy", {}).get("enabled", True)
              inputs["run_docker"] = tools.get("docker", {}).get("enabled", False)

              # Python-specific thresholds
              threshold_bundle["coverage_min"] = tools.get("pytest", {}).get("min_coverage", 70)
              threshold_bundle["mutation_score_min"] = tools.get("mutmut", {}).get("min_mutation_score", 70)
              # For Trivy CVSS gate - check trivy config or default to 7
              threshold_bundle["owasp_cvss_fail"] = tools.get("trivy", {}).get("fail_on_cvss", 7)
              threshold_bundle["max_ruff_errors"] = tools.get("ruff", {}).get("max_errors", 0)
              threshold_bundle["max_black_issues"] = tools.get("black", {}).get("max_issues", 0)
              threshold_bundle["max_isort_issues"] = tools.get("isort", {}).get("max_issues", 0)
              threshold_bundle["max_semgrep_findings"] = tools.get("semgrep", {}).get("max_findings", 0)

          # Global thresholds override tool-specific (ADR-0024)
          if "coverage_min" in global_thresholds:
              threshold_bundle["coverage_min"] = global_thresholds["coverage_min"]
          if "mutation_score_min" in global_thresholds:
              threshold_bundle["mutation_score_min"] = global_thresholds["mutation_score_min"]
          if "owasp_cvss_fail" in global_thresholds:
              threshold_bundle["owasp_cvss_fail"] = global_thresholds["owasp_cvss_fail"]

          # Bundle as YAML string for dispatch
          inputs["threshold_overrides_yaml"] = yaml.dump(threshold_bundle, default_flow_style=False)

          # NOTE: retention_days and artifact_prefix removed from dispatch
          # Caller template has sensible defaults; edit there if needed
          force_all_tools = repo_meta.get("force_all_tools", False)
          inputs["force_all_tools"] = force_all_tools

          dispatch_workflow = repo_meta.get("dispatch_workflow") or os.environ.get("MATRIX_DISPATCH_WORKFLOW", "")
          dispatch_enabled = repo_meta.get("dispatch_enabled", os.environ.get("MATRIX_DISPATCH_ENABLED", "true"))

          if force_all_tools:
              if language == "java":
                  for key in [
                      "run_jacoco",
                      "run_checkstyle",
                      "run_spotbugs",
                      "run_owasp",
                      "run_pitest",
                      "run_jqwik",
                      "run_codeql",
                      "run_pmd",
                      "run_semgrep",
                      "run_trivy",
                  ]:
                      inputs[key] = True
              elif language == "python":
                  for key in [
                      "run_pytest",
                      "run_ruff",
                      "run_bandit",
                      "run_pip_audit",
                      "run_mypy",
                      "run_codeql",
                      "run_black",
                      "run_isort",
                      "run_mutmut",
                      "run_hypothesis",
                      "run_semgrep",
                      "run_trivy",
                  ]:
                      inputs[key] = True

          # Write outputs - use heredoc for multi-line values (threshold_overrides_yaml)
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              for key, value in inputs.items():
                  if key == "threshold_overrides_yaml":
                      # Multi-line YAML requires heredoc syntax
                      f.write(f"{key}<<EOF\n{value}EOF\n")
                  elif isinstance(value, bool):
                      f.write(f"{key}={str(value).lower()}\n")
                  else:
                      f.write(f"{key}={value}\n")
              f.write(f"dispatch_workflow={dispatch_workflow}\n")
              f.write(f"dispatch_enabled={str(dispatch_enabled).lower()}\n")

          print(f"Config for ${{ matrix.name }}:")
          print(json.dumps(inputs, indent=2))
          EOF

      - name: Trigger Repository Workflow
        id: dispatch
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.HUB_DISPATCH_TOKEN || secrets.GITHUB_TOKEN }}
          script: |
            const owner = '${{ matrix.owner }}';
            const repo = '${{ matrix.name }}';
            const language = '${{ matrix.language }}';
            const branch = '${{ matrix.default_branch }}';
            const workdir = '${{ steps.repo-config.outputs.workdir }}';
            const dispatchEnabled = '${{ steps.repo-config.outputs.dispatch_enabled || '' }}'.toLowerCase() !== 'false';
            const correlationId = `${{ github.run_id }}-${{ github.run_attempt }}-${{ matrix.config_basename }}`;

            if (!dispatchEnabled) {
              core.info(`Dispatch disabled for ${owner}/${repo}; skipping dispatch.`);
              return;
            }

            // GitHub workflow_dispatch API requires ALL inputs as strings
            const asBool = (val) => String(val || '').toLowerCase() === 'true' ? 'true' : 'false';
            const asNumber = (val, fallback) => {
              const n = Number(val);
              return String(Number.isFinite(n) ? n : fallback);
            };
            const forceAllTools = ('${{ steps.repo-config.outputs.force_all_tools }}' || 'false').toLowerCase() === 'true';

            const inputs = { hub_correlation_id: correlationId };
            const MAX_POLL_MS = 30 * 60 * 1000; // 30 minutes

            if (!['java', 'python'].includes(language)) {
              core.setFailed(`Unsupported language '${language}' in config for ${owner}/${repo}`);
              return;
            }

            // Threshold overrides bundled as YAML (ADR-0024)
            // Use template literal for multi-line YAML (heredoc output preserves newlines)
            const thresholdOverrides = `${{ steps.repo-config.outputs.threshold_overrides_yaml }}`.trim();
            if (thresholdOverrides) {
              inputs.threshold_overrides_yaml = thresholdOverrides;
            }

            if (language === 'java') {
              inputs.java_version = '${{ steps.repo-config.outputs.java_version }}' || '21';
              inputs.build_tool = '${{ steps.repo-config.outputs.build_tool }}' || 'maven';
              inputs.run_jacoco = asBool('${{ steps.repo-config.outputs.run_jacoco }}');
              inputs.run_checkstyle = asBool('${{ steps.repo-config.outputs.run_checkstyle }}');
              inputs.run_spotbugs = asBool('${{ steps.repo-config.outputs.run_spotbugs }}');
              inputs.run_owasp = asBool('${{ steps.repo-config.outputs.run_owasp }}');
              inputs.use_nvd_api_key = asBool('${{ steps.repo-config.outputs.use_nvd_api_key }}');
              inputs.run_pitest = asBool('${{ steps.repo-config.outputs.run_pitest }}');
              inputs.run_jqwik = asBool('${{ steps.repo-config.outputs.run_jqwik }}');
              inputs.run_codeql = asBool('${{ steps.repo-config.outputs.run_codeql }}');
              inputs.run_pmd = asBool('${{ steps.repo-config.outputs.run_pmd }}');
              inputs.run_semgrep = asBool('${{ steps.repo-config.outputs.run_semgrep }}');
              inputs.run_trivy = asBool('${{ steps.repo-config.outputs.run_trivy }}');
              inputs.run_docker = asBool('${{ steps.repo-config.outputs.run_docker }}');
              if (workdir) {
                inputs.workdir = workdir;
              }
              if (forceAllTools) {
                [
                  "run_jacoco",
                  "run_checkstyle",
                  "run_spotbugs",
                  "run_owasp",
                  "run_pitest",
                  "run_jqwik",
                  "run_codeql",
                  "run_pmd",
                  "run_semgrep",
                  "run_trivy",
                  "run_docker",
                ].forEach((key) => {
                  inputs[key] = 'true';
                });
              }
            } else {
              inputs.python_version = '${{ steps.repo-config.outputs.python_version }}' || '3.12';
              inputs.run_pytest = asBool('${{ steps.repo-config.outputs.run_pytest }}');
              inputs.run_ruff = asBool('${{ steps.repo-config.outputs.run_ruff }}');
              inputs.run_bandit = asBool('${{ steps.repo-config.outputs.run_bandit }}');
              inputs.run_pip_audit = asBool('${{ steps.repo-config.outputs.run_pip_audit }}');
              inputs.run_mypy = asBool('${{ steps.repo-config.outputs.run_mypy }}');
              inputs.run_codeql = asBool('${{ steps.repo-config.outputs.run_codeql }}');
              inputs.run_black = asBool('${{ steps.repo-config.outputs.run_black }}');
              inputs.run_isort = asBool('${{ steps.repo-config.outputs.run_isort }}');
              inputs.run_mutmut = asBool('${{ steps.repo-config.outputs.run_mutmut }}');
              inputs.run_hypothesis = asBool('${{ steps.repo-config.outputs.run_hypothesis }}');
              inputs.run_semgrep = asBool('${{ steps.repo-config.outputs.run_semgrep }}');
              inputs.run_trivy = asBool('${{ steps.repo-config.outputs.run_trivy }}');
              inputs.run_docker = asBool('${{ steps.repo-config.outputs.run_docker }}');
              if (workdir) {
                inputs.workdir = workdir;
              }
              if (forceAllTools) {
                [
                  "run_pytest",
                  "run_ruff",
                  "run_bandit",
                  "run_pip_audit",
                  "run_mypy",
                  "run_codeql",
                  "run_black",
                  "run_isort",
                  "run_mutmut",
                  "run_hypothesis",
                  "run_semgrep",
                  "run_trivy",
                  "run_docker",
                ].forEach((key) => {
                  inputs[key] = 'true';
                });
              }
            }

            // Use configurable workflow name from matrix, with fallback to new hub-*-ci.yml (reusable workflow callers)
            const workflowId = '${{ steps.repo-config.outputs.dispatch_workflow }}' || (language === 'java' ? 'hub-java-ci.yml' : 'hub-python-ci.yml');
            const startedAt = Date.now();

            core.info(`Dispatching ${workflowId} on ${owner}/${repo}@${branch}`);
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner,
                repo,
                workflow_id: workflowId,
                ref: branch,
                inputs,
              });
            } catch (err) {
              core.setFailed(`Dispatch failed for ${owner}/${repo}: ${err.message}`);
              throw err;
            }

            let runId = '';
            let pollDelay = 5000;
            const deadline = startedAt + MAX_POLL_MS;

            while (Date.now() < deadline) {
              await new Promise((resolve) => setTimeout(resolve, pollDelay));
              const runs = await github.rest.actions.listWorkflowRuns({
                owner,
                repo,
                workflow_id: workflowId,
                event: 'workflow_dispatch',
                branch,
                per_page: 5,
              });

            const recent = runs.data.workflow_runs.find((run) => {
                const created = new Date(run.created_at).getTime();
                return (
                  created >= startedAt - 2000 && // tighter window to reduce collisions
                  run.head_branch === branch &&
                  run.status !== 'completed'
                );
              });

              if (recent) {
                runId = String(recent.id);
                core.info(`Captured run id ${runId} for ${repo}`);
                break;
              }

              pollDelay = Math.min(pollDelay * 2, 30000); // backoff up to 30s
            }

            if (!runId) {
              core.setFailed(`Dispatched ${workflowId} for ${repo}, but could not determine run id.`);
              return;
            }

            core.setOutput('run_id', runId);
            core.setOutput('branch', branch);
            core.setOutput('workflow_id', workflowId);
            core.setOutput('correlation_id', correlationId);

      - name: Record Build Trigger
        run: |
          echo "## ${{ matrix.name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Owner:** ${{ matrix.owner }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Language:** ${{ matrix.language }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ matrix.default_branch }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow:** ${{ steps.dispatch.outputs.workflow_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID:** ${{ steps.dispatch.outputs.run_id != '' && steps.dispatch.outputs.run_id || 'pending' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** Triggered" >> $GITHUB_STEP_SUMMARY

      - name: Save Dispatch Metadata
        if: always()
        run: |
          mkdir -p dispatch-results
          cat > dispatch-results/${{ matrix.config_basename }}.json << EOF
          {
            "config": "${{ matrix.config_basename }}",
            "repo": "${{ matrix.owner }}/${{ matrix.name }}",
            "subdir": "${{ matrix.subdir }}",
            "language": "${{ matrix.language }}",
            "branch": "${{ matrix.default_branch }}",
            "workflow": "${{ steps.dispatch.outputs.workflow_id }}",
            "artifact_prefix": "${{ steps.repo-config.outputs.artifact_prefix }}",
            "force_all_tools": "${{ steps.repo-config.outputs.force_all_tools }}",
            "run_id": "${{ steps.dispatch.outputs.run_id }}",
            "correlation_id": "${{ steps.dispatch.outputs.correlation_id }}",
            "dispatch_time": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "status": "${{ job.status }}"
          }
          EOF

      - name: Upload Dispatch Metadata
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dispatch-${{ matrix.config_basename }}-${{ github.run_id }}
          path: dispatch-results/${{ matrix.config_basename }}.json
          retention-days: 7

  # ============================================================================
  # Aggregate Reports
  # ============================================================================
  aggregate-reports:
    name: Aggregate Reports
    runs-on: ubuntu-latest
    needs: [load-config, trigger-builds]
    if: always()

    steps:
      - name: Checkout Hub
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Dependencies
        run: pip install pyyaml

      - name: Download Dispatch Metadata
        uses: actions/download-artifact@v4
        with:
          path: dispatch-artifacts
        continue-on-error: true

      - name: Generate Hub Summary and Report
        env:
          TOTAL_REPOS: ${{ needs.load-config.outputs.repo_count }}
          HUB_RUN_ID: ${{ github.run_id }}
          HUB_EVENT: ${{ github.event_name }}
          # Use HUB_DISPATCH_TOKEN for cross-repo artifact access, fallback to GITHUB_TOKEN
          GITHUB_TOKEN: ${{ secrets.HUB_DISPATCH_TOKEN || secrets.GITHUB_TOKEN }}
        run: |
          python << 'EOF'
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          from urllib import request
          import time
          import zipfile
          import tempfile

          token = os.environ.get("GITHUB_TOKEN")
          if not token:
              print("Missing GITHUB_TOKEN for aggregation", flush=True)
              exit(1)

          def gh_get(url: str, retries: int = 3, backoff: float = 2.0) -> dict:
              attempt = 0
              while True:
                  try:
                      req = request.Request(
                          url,
                          headers={
                              "Authorization": f"Bearer {token}",
                              "Accept": "application/vnd.github+json",
                              "X-GitHub-Api-Version": "2022-11-28",
                          },
                      )
                      with request.urlopen(req) as resp:
                          return json.loads(resp.read().decode())
                  except Exception as exc:  # noqa: PIE786
                      attempt += 1
                      if attempt > retries:
                          raise
                      sleep_for = backoff * attempt
                      print(f"Retry {attempt}/{retries} for {url} after error: {exc} (sleep {sleep_for}s)")
                      time.sleep(sleep_for)

          def download_artifact(archive_url: str, target_dir: Path) -> Path | None:
              req = request.Request(
                  archive_url,
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Accept": "application/vnd.github+json",
                      "X-GitHub-Api-Version": "2022-11-28",
                  },
              )
              try:
                  with request.urlopen(req) as resp:
                      data = resp.read()
                  target_dir.mkdir(parents=True, exist_ok=True)
                  zip_path = target_dir / "artifact.zip"
                  zip_path.write_bytes(data)
                  with zipfile.ZipFile(zip_path, "r") as zf:
                      zf.extractall(target_dir)
                  return target_dir
              except Exception as exc:  # noqa: PIE786
                  print(f"Warning: failed to download artifact {archive_url}: {exc}")
                  return None

          meta_dir = Path("dispatch-artifacts")
          entries = []
          for path in meta_dir.rglob("*.json"):
              try:
                  data = json.loads(path.read_text())
                  data["_source"] = str(path)
                  entries.append(data)
              except Exception as exc:  # noqa: PIE786
                  print(f"Warning: could not read {path}: {exc}")

          def find_run_by_correlation_id(owner: str, repo: str, workflow_id: str, correlation_id: str) -> str | None:
              """
              Deterministic run matching: search recent runs and match by hub_correlation_id in artifact.
              Returns run_id if found, None otherwise.
              """
              if not correlation_id:
                  return None
              try:
                  # List recent workflow runs
                  runs_url = f"https://api.github.com/repos/{owner}/{repo}/actions/workflows/{workflow_id}/runs?per_page=20&event=workflow_dispatch"
                  runs_data = gh_get(runs_url)
                  for run in runs_data.get("workflow_runs", []):
                      run_id = run.get("id")
                      if not run_id:
                          continue
                      # Check artifacts for this run
                      try:
                          artifacts = gh_get(f"https://api.github.com/repos/{owner}/{repo}/actions/runs/{run_id}/artifacts")
                          ci_artifact = next((a for a in artifacts.get("artifacts", []) if a.get("name", "").endswith("ci-report")), None)
                          if ci_artifact:
                              with tempfile.TemporaryDirectory() as tmpdir:
                                  extracted = download_artifact(ci_artifact["archive_download_url"], Path(tmpdir))
                                  if extracted:
                                      report_file = next(iter(Path(extracted).rglob("report.json")), None)
                                      if report_file and report_file.exists():
                                          report_data = json.loads(report_file.read_text())
                                          if report_data.get("hub_correlation_id") == correlation_id:
                                              print(f"Found matching run {run_id} for correlation_id {correlation_id}")
                                              return str(run_id)
                      except Exception as e:
                          print(f"Warning: error checking run {run_id} artifacts: {e}")
                          continue
              except Exception as e:
                  print(f"Warning: error searching runs for correlation_id {correlation_id}: {e}")
              return None

          # Poll runs and collect artifacts/metrics
          results = []
          pending_statuses = {"queued", "in_progress", "waiting", "pending"}
          poll_timeout_sec = 30 * 60  # 30 minutes
          for entry in entries:
              repo_full = entry.get("repo", "unknown/unknown")
              owner_repo = repo_full.split("/")
              if len(owner_repo) != 2:
                  print(f"Invalid repo format in entry: {repo_full}")
                  continue
              owner, repo = owner_repo
              run_id = entry.get("run_id")
              language = entry.get("language")
              branch = entry.get("branch")
              workflow = entry.get("workflow")

              run_status = {
                  "config": entry.get("config", repo),
                  "repo": repo_full,
                  "subdir": entry.get("subdir", ""),
                  "language": language,
                  "branch": branch,
                  "workflow": workflow,
                  "run_id": run_id or "",
                  "correlation_id": entry.get("correlation_id", ""),
                  "status": "missing_run_id" if not run_id else "unknown",
                  "conclusion": "unknown",
                  # Common quality metrics
                  "coverage": None,
                  "mutation_score": None,
                  # Java-specific tools
                  "checkstyle_issues": None,
                  "spotbugs_issues": None,
                  "pmd_violations": None,
                  "owasp_critical": None,
                  "owasp_high": None,
                  "owasp_medium": None,
                  # Python-specific tools
                  "tests_passed": None,
                  "tests_failed": None,
                  "ruff_errors": None,
                  "black_issues": None,
                  "isort_issues": None,
                  "mypy_errors": None,
                  "bandit_high": None,
                  "bandit_medium": None,
                  "pip_audit_vulns": None,
                  # Cross-language security tools
                  "semgrep_findings": None,
                  "trivy_critical": None,
                  "trivy_high": None,
                  # Track which tools ran
                  "tools_ran": {},
              }

              # Deterministic correlation: if run_id is missing, search by correlation_id
              expected_corr = entry.get("correlation_id", "")
              if not run_id and expected_corr and workflow:
                  print(f"No run_id for {repo_full}, searching by correlation_id {expected_corr}...")
                  found_run_id = find_run_by_correlation_id(owner, repo, workflow, expected_corr)
                  if found_run_id:
                      run_id = found_run_id
                      run_status["run_id"] = run_id
                      run_status["status"] = "unknown"  # Reset status, will be updated by polling
                      print(f"Found run_id {run_id} for {repo_full} via correlation_id")
                  else:
                      print(f"Could not find run by correlation_id for {repo_full}")

              if not run_id:
                  results.append(run_status)
                  continue

              try:
                  url = f"https://api.github.com/repos/{owner}/{repo}/actions/runs/{run_id}"
                  start_poll = time.time()
                  delay = 10
                  while True:
                      run = gh_get(url)
                      status = run.get("status", "unknown")
                      conclusion = run.get("conclusion", "unknown")
                      run_status["status"] = status
                      run_status["conclusion"] = conclusion

                      if status not in pending_statuses:
                          break

                      if time.time() - start_poll > poll_timeout_sec:
                          run_status["status"] = "timed_out"
                          run_status["conclusion"] = "timed_out"
                          break

                      time.sleep(delay)
                      delay = min(delay * 1.5, 60)
              except Exception as exc:  # noqa: PIE786
                  print(f"Warning: could not fetch run {run_id} for {repo_full}: {exc}")
                  run_status["status"] = "fetch_failed"
                  results.append(run_status)
                  continue

              # If completed and succeeded, try to download artifacts
              if run_status["status"] == "completed" and run_status["conclusion"] == "success":
                  try:
                      artifacts = gh_get(f"https://api.github.com/repos/{owner}/{repo}/actions/runs/{run_id}/artifacts")
                      ci_artifacts = artifacts.get("artifacts", [])
                      # Prefer artifact ending with ci-report (handles prefix like 'python-passing-ci-report')
                      artifact = next((a for a in ci_artifacts if a.get("name", "").endswith("ci-report")), None)
                      if not artifact and ci_artifacts:
                          # Fallback to first artifact containing 'report'
                          artifact = next((a for a in ci_artifacts if "report" in a.get("name", "")), ci_artifacts[0])
                      if artifact:
                          with tempfile.TemporaryDirectory() as tmpdir:
                              extracted = download_artifact(artifact["archive_download_url"], Path(tmpdir))
                              if extracted:
                                  report_file = next(iter(Path(extracted).rglob("report.json")), None)
                                  if report_file and report_file.exists():
                                      try:
                                          report_data = json.loads(report_file.read_text())
                                          report_corr = report_data.get("hub_correlation_id", "")
                                          run_status["correlation_id"] = report_corr or expected_corr
                                          if expected_corr and report_corr and report_corr != expected_corr:
                                              print(
                                                  f"Correlation mismatch for {repo_full} run {run_id} "
                                                  f"(expected {expected_corr}, got {report_corr}); searching for correct run..."
                                              )
                                              # Try to find the correct run by correlation ID
                                              correct_run_id = find_run_by_correlation_id(owner, repo, workflow, expected_corr)
                                              if correct_run_id and correct_run_id != run_id:
                                                  print(f"Found correct run {correct_run_id}, re-fetching artifacts...")
                                                  # Re-fetch artifact from correct run
                                                  correct_artifacts = gh_get(f"https://api.github.com/repos/{owner}/{repo}/actions/runs/{correct_run_id}/artifacts")
                                                  correct_ci_artifact = next((a for a in correct_artifacts.get("artifacts", []) if a.get("name", "").endswith("ci-report")), None)
                                                  if correct_ci_artifact:
                                                      with tempfile.TemporaryDirectory() as tmpdir2:
                                                          extracted2 = download_artifact(correct_ci_artifact["archive_download_url"], Path(tmpdir2))
                                                          if extracted2:
                                                              report_file2 = next(iter(Path(extracted2).rglob("report.json")), None)
                                                              if report_file2 and report_file2.exists():
                                                                  report_data = json.loads(report_file2.read_text())
                                                                  run_status["run_id"] = correct_run_id
                                                                  run_status["correlation_id"] = expected_corr
                                                                  # Continue processing with correct report_data
                                              else:
                                                  print(f"Could not find correct run for {repo_full}, skipping artifact.")
                                                  continue
                                          results_data = report_data.get("results", {}) or {}
                                          tool_metrics = report_data.get("tool_metrics", {}) or {}
                                          # Common quality metrics
                                          run_status["coverage"] = results_data.get("coverage")
                                          run_status["mutation_score"] = results_data.get("mutation_score")
                                          # Java-specific tools
                                          run_status["checkstyle_issues"] = tool_metrics.get("checkstyle_issues")
                                          run_status["spotbugs_issues"] = tool_metrics.get("spotbugs_issues")
                                          run_status["pmd_violations"] = tool_metrics.get("pmd_violations")
                                          run_status["owasp_critical"] = tool_metrics.get("owasp_critical")
                                          run_status["owasp_high"] = tool_metrics.get("owasp_high")
                                          run_status["owasp_medium"] = tool_metrics.get("owasp_medium")
                                          # Python-specific tools
                                          run_status["tests_passed"] = results_data.get("tests_passed")
                                          run_status["tests_failed"] = results_data.get("tests_failed")
                                          run_status["ruff_errors"] = tool_metrics.get("ruff_errors")
                                          run_status["black_issues"] = tool_metrics.get("black_issues")
                                          run_status["isort_issues"] = tool_metrics.get("isort_issues")
                                          run_status["mypy_errors"] = tool_metrics.get("mypy_errors")
                                          run_status["bandit_high"] = tool_metrics.get("bandit_high")
                                          run_status["bandit_medium"] = tool_metrics.get("bandit_medium")
                                          run_status["pip_audit_vulns"] = tool_metrics.get("pip_audit_vulns")
                                          # Cross-language security tools
                                          run_status["semgrep_findings"] = tool_metrics.get("semgrep_findings")
                                          run_status["trivy_critical"] = tool_metrics.get("trivy_critical")
                                          run_status["trivy_high"] = tool_metrics.get("trivy_high")
                                          # Track which tools ran
                                          run_status["tools_ran"] = report_data.get("tools_ran", {})
                                      except Exception as exc:  # noqa: PIE786
                                          print(f"Warning: could not parse report.json from {artifact['name']}: {exc}")
                  except Exception as exc:  # noqa: PIE786
                      print(f"Warning: failed to fetch artifacts for run {run_id} in {repo_full}: {exc}")

              results.append(run_status)

          total_repos = int(os.environ.get("TOTAL_REPOS") or 0)
          dispatched = len(results)
          missing = max(total_repos - dispatched, 0)
          missing_run_id = len([e for e in results if not e.get("run_id")])

          failed_runs = [
              r for r in results
              if r.get("status") in ("missing_run_id", "fetch_failed")
              or (r.get("status") == "completed" and r.get("conclusion") != "success")
              or r.get("status") not in ("completed",)
          ]

          report = {
              "hub_run_id": os.environ.get("HUB_RUN_ID"),
              "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "triggered_by": os.environ.get("HUB_EVENT"),
              "total_repos": total_repos,
              "dispatched_repos": dispatched,
              "missing_dispatch_metadata": missing,
              "runs": results,
          }

          # Helper to collect numeric values (ignores None)
          def collect_values(key):
              return [r[key] for r in results if isinstance(r.get(key), (int, float))]

          # Aggregated quality metrics
          coverages = collect_values("coverage")
          mutations = collect_values("mutation_score")

          if coverages:
              report["coverage_average"] = round(sum(coverages) / len(coverages), 1)
          if mutations:
              report["mutation_average"] = round(sum(mutations) / len(mutations), 1)

          # Aggregated vulnerability metrics (OWASP for Java, Bandit/pip-audit for Python, Trivy/Semgrep for both)
          owasp_critical = collect_values("owasp_critical")
          owasp_high = collect_values("owasp_high")
          owasp_medium = collect_values("owasp_medium")
          bandit_high = collect_values("bandit_high")
          bandit_medium = collect_values("bandit_medium")
          pip_audit_vulns = collect_values("pip_audit_vulns")
          trivy_critical = collect_values("trivy_critical")
          trivy_high = collect_values("trivy_high")
          semgrep_findings = collect_values("semgrep_findings")

          # Total critical/high across all security tools
          report["total_critical_vulns"] = sum(owasp_critical) + sum(trivy_critical)
          report["total_high_vulns"] = sum(owasp_high) + sum(bandit_high) + sum(trivy_high)
          report["total_medium_vulns"] = sum(owasp_medium) + sum(bandit_medium)
          report["total_pip_audit_vulns"] = sum(pip_audit_vulns)
          report["total_semgrep_findings"] = sum(semgrep_findings)

          # Aggregated code quality metrics
          checkstyle_issues = collect_values("checkstyle_issues")
          spotbugs_issues = collect_values("spotbugs_issues")
          pmd_violations = collect_values("pmd_violations")
          ruff_errors = collect_values("ruff_errors")
          black_issues = collect_values("black_issues")
          isort_issues = collect_values("isort_issues")
          mypy_errors = collect_values("mypy_errors")

          report["total_code_quality_issues"] = (
              sum(checkstyle_issues) + sum(spotbugs_issues) + sum(pmd_violations) +
              sum(ruff_errors) + sum(black_issues) + sum(isort_issues) + sum(mypy_errors)
          )

          Path("hub-report.json").write_text(json.dumps(report, indent=2))

          # Helper to format metric (None -> "-", else value)
          def fmt(val, suffix=""):
              return f"{val}{suffix}" if val is not None else "-"

          summary_lines = [
              "# CI/CD Hub Report",
              "",
              f"**Run ID:** {report['hub_run_id']}",
              f"**Timestamp:** {report['timestamp']}",
              "",
              "## Dispatch Status",
              f"- Total configs: {total_repos}",
              f"- Successfully dispatched: {dispatched}",
              f"- Missing metadata: {missing}",
              f"- Missing run IDs: {missing_run_id}",
              "",
          ]

          # Separate Java and Python results
          java_results = [r for r in results if r.get("language") == "java"]
          python_results = [r for r in results if r.get("language") == "python"]

          # Java Results Table
          if java_results:
              summary_lines.extend([
                  "## Java Repos",
                  "",
                  "| Config | Status | Cov | Mut | CS | SB | PMD | OWASP | Semgrep | Trivy |",
                  "|--------|--------|-----|-----|----|----|-----|-------|---------|-------|",
              ])
              for entry in java_results:
                  config = entry.get('config', entry.get('repo', 'unknown').split('/')[-1])
                  status = entry.get('conclusion', entry.get('status', 'unknown'))
                  status_label = "PASS" if status == "success" else "FAIL" if status in ("failure", "failed") else "PENDING"

                  cov = fmt(entry.get('coverage'), '%')
                  mut = fmt(entry.get('mutation_score'), '%')
                  cs = fmt(entry.get('checkstyle_issues'))
                  sb = fmt(entry.get('spotbugs_issues'))
                  pmd = fmt(entry.get('pmd_violations'))

                  oc = entry.get('owasp_critical')
                  oh = entry.get('owasp_high')
                  om = entry.get('owasp_medium')
                  owasp = f"{oc or 0}/{oh or 0}/{om or 0}" if any(v is not None for v in [oc, oh, om]) else "-"

                  sem = fmt(entry.get('semgrep_findings'))

                  tc = entry.get('trivy_critical')
                  th = entry.get('trivy_high')
                  trivy = f"{tc or 0}/{th or 0}" if any(v is not None for v in [tc, th]) else "-"

                  summary_lines.append(f"| {config} | {status_label} | {cov} | {mut} | {cs} | {sb} | {pmd} | {owasp} | {sem} | {trivy} |")
              summary_lines.append("")

          # Python Results Table
          if python_results:
              summary_lines.extend([
                  "## Python Repos",
                  "",
                  "| Config | Status | Cov | Mut | Tests | Ruff | Black | isort | mypy | Bandit | pip-audit | Semgrep | Trivy |",
                  "|--------|--------|-----|-----|-------|------|-------|-------|------|--------|-----------|---------|-------|",
              ])
              for entry in python_results:
                  config = entry.get('config', entry.get('repo', 'unknown').split('/')[-1])
                  status = entry.get('conclusion', entry.get('status', 'unknown'))
                  status_label = "PASS" if status == "success" else "FAIL" if status in ("failure", "failed") else "PENDING"

                  cov = fmt(entry.get('coverage'), '%')
                  mut = fmt(entry.get('mutation_score'), '%')

                  tp = entry.get('tests_passed')
                  tf = entry.get('tests_failed')
                  tests = f"{tp or 0} pass/{tf or 0} fail" if any(v is not None for v in [tp, tf]) else "-"

                  ruff = fmt(entry.get('ruff_errors'))
                  black = fmt(entry.get('black_issues'))
                  isort = fmt(entry.get('isort_issues'))
                  mypy = fmt(entry.get('mypy_errors'))

                  bh = entry.get('bandit_high')
                  bm = entry.get('bandit_medium')
                  bandit = f"{bh or 0}/{bm or 0}" if any(v is not None for v in [bh, bm]) else "-"

                  pip = fmt(entry.get('pip_audit_vulns'))
                  sem = fmt(entry.get('semgrep_findings'))

                  tc = entry.get('trivy_critical')
                  th = entry.get('trivy_high')
                  trivy = f"{tc or 0}/{th or 0}" if any(v is not None for v in [tc, th]) else "-"

                  summary_lines.append(f"| {config} | {status_label} | {cov} | {mut} | {tests} | {ruff} | {black} | {isort} | {mypy} | {bandit} | {pip} | {sem} | {trivy} |")
              summary_lines.append("")

          # Aggregated Metrics
          summary_lines.extend([
              "## Aggregated Metrics",
              "",
              "### Quality",
          ])
          if "coverage_average" in report:
              summary_lines.append(f"- **Average Coverage:** {report['coverage_average']}%")
          if "mutation_average" in report:
              summary_lines.append(f"- **Average Mutation Score:** {report['mutation_average']}%")
          summary_lines.append(f"- **Total Code Quality Issues:** {report['total_code_quality_issues']}")

          summary_lines.extend([
              "",
              "### Security",
              f"- **Critical Vulnerabilities (OWASP+Trivy):** {report['total_critical_vulns']}",
              f"- **High Vulnerabilities (OWASP+Bandit+Trivy):** {report['total_high_vulns']}",
              f"- **Medium Vulnerabilities (OWASP+Bandit):** {report['total_medium_vulns']}",
              f"- **pip-audit Vulnerabilities:** {report['total_pip_audit_vulns']}",
              f"- **Semgrep Findings:** {report['total_semgrep_findings']}",
          ])

          summary_path = Path(os.environ["GITHUB_STEP_SUMMARY"])
          summary_path.write_text("\n".join(summary_lines))

          # Load thresholds from defaults.yaml
          import yaml
          max_critical_vulns = 0
          max_high_vulns = 0
          defaults_path = Path("config/defaults.yaml")
          if defaults_path.exists():
              try:
                  defaults = yaml.safe_load(defaults_path.read_text())
                  thresholds = defaults.get("thresholds", {})
                  max_critical_vulns = thresholds.get("max_critical_vulns", 0)
                  max_high_vulns = thresholds.get("max_high_vulns", 0)
              except Exception as exc:  # noqa: PIE786
                  print(f"Warning: could not load thresholds from defaults.yaml: {exc}")

          # Check vulnerability thresholds
          threshold_exceeded = False
          if report["total_critical_vulns"] > max_critical_vulns:
              print(f"THRESHOLD EXCEEDED: Critical vulnerabilities {report['total_critical_vulns']} > {max_critical_vulns}", flush=True)
              threshold_exceeded = True
          if report["total_high_vulns"] > max_high_vulns:
              print(f"THRESHOLD EXCEEDED: High vulnerabilities {report['total_high_vulns']} > {max_high_vulns}", flush=True)
              threshold_exceeded = True

          # Fail the job if any run failed, metadata is missing, or thresholds exceeded
          if failed_runs or missing > 0 or threshold_exceeded:
              if failed_runs:
                  print("Aggregation detected failures.", flush=True)
              if missing > 0:
                  print("Aggregation detected missing data.", flush=True)
              if threshold_exceeded:
                  print("Vulnerability thresholds exceeded.", flush=True)
              print("Failing job.", flush=True)
              exit(1)
          EOF

      - name: Upload Hub Report
        uses: actions/upload-artifact@v4
        with:
          name: hub-report-${{ github.run_id }}
          path: hub-report.json
          retention-days: 30
